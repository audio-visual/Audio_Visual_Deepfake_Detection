dataset_name: deepfake_video_audio
train_split: ['train']
val_split: ['dev']
test_split: ['test']
devices: ['cuda:1']
dataset: {
  
    video_feat_folder: /home/ubuntu/sn15_share_dir/av_deepfake/train_frame_features,
    audio_feat_folder: /home/ubuntu/sn15_share_dir/av_deepfake/train_wav,
    train_txt: /home/ubuntu/sn15_share_dir/av_deepfake/preprocess_v2/train_rela_339552_equal_json_less20.txt,
    json_folder: /home/ubuntu/sn15_share_dir/av_deepfake/train_metadata,
    feat_stride: 1,
    num_frames: 1,
    default_fps: None,
    downsample_rate: 0,
    max_seq_len: 768, 
    trunc_thresh: 0.5,
    crop_ratio: [0.9,1.0],
    video_input_dim: 256,
    audio_input_dim: 768,
    num_classes: 1,
    file_prefix: rgb,
    file_ext: .npy,
    audio_file_ext: .npy,
    force_upsampling: True
}

model_name: AVLocPointTransformerRecoveryNoNorm
model: {
  backbone_type: convHRLRFullResSelfAttTransformerRevised,
  fpn_type: fpn,
  max_buffer_len_factor: 1.0,
  # 192 - 96 - 48 - 24 - 12 - 6
  n_mha_win_size: [7, 7, 7, 7, 7, -1],
  # shrink the model for reduced input feature channels
  n_head: 4,
  embd_dim: 256,
  fpn_dim: 256,
  head_dim: 256,
  use_abs_pe: True, # TODO
}
opt: {
  learning_rate: 0.001,
  epochs: 15,
  weight_decay: 0.05,
}
loader: {
  batch_size: 72,
}
train_cfg: {
  init_loss_norm: 200,
  clip_grad_l2norm: 1.0,
  cls_prior_prob: 0.01,
  center_sample: radius,
  center_sample_radius: 1.5,
  label_smoothing: 0.1,
  droppath: 0.1,
  loss_weight: 2.0,
}

# similar to THUMOS
test_cfg: {
  voting_thresh: 0.9,
  pre_nms_topk: 2000,
  # max of 100 predictions per video
  max_seg_num: 100,
  min_score: 0.001,
  # score fusion
  multiclass_nms: False,
  nms_sigma : 0.75,
  # ext_score_file: None,
  duration_thresh: 0.001,
}
output_folder: /home/ubuntu/sn15_share_dir/av_deepfake/train_results/exp5
